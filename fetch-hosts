#!/bin/sh

#
#  ----------------------------------------------------
#  vsm - vector space model data similarity
#  ----------------------------------------------------
#
#  Copyright (c) 2008 Jason Bittel <jason.bittel@gmail.com>
#
#
# This script takes a file containing newline delimited URLs. The textual data
# from each URL is fetched, the text is normalized, and then processed with vsm.
# The text, along with its associated log file, is written into a data directory
# and a DB file is updated accordingly. At the start of each run, the DB file is
# checked and old pages are removed as necessary.
#

if [ "$#" -ne 3 ] ; then
        echo "Error: Malformed parameter list" >&2
        echo "Usage: ${0} <url-file> <term-file> <data-dir>" >&2
        echo "" >&2
        exit 1
fi

url_file=${1}
term_file=${2}
dir=${3%/}

curl_bin="/usr/bin/curl"
lynx_bin="/usr/bin/lynx"
vsm_bin="./vsm"

if [ ! -x ${curl_bin} ] ; then
        echo "Error: Cannot find or access curl at '${curl_bin}'" >&2
        exit 1
fi

if [ ! -x ${lynx_bin} ] ; then
        echo "Error: Cannot find or access lynx at '${lynx_bin}'" >&2
        exit 1
fi

if [ ! -x ${vsm_bin} ] ; then
        echo "Error: Cannot find or access vsm at '${vsm_bin}'" >&2
        exit 1
fi

if [ ! -r "${url_file}" ] ; then
        echo "Error: Cannot read from file '${url_file}'" >&2
        exit 1
fi

if [ ! -r "${term_file}" ] ; then
        echo "Error: Cannot read from file '${term_file}'" >&2
        exit 1
fi

if [ ! -d "${dir}" ] ; then
        mkdir "${dir}"
fi
touch "${dir}/fetched"

# Purge expired pages from DB as necessary
rm -f "${dir}/fetched-2"
while read hostname filename date_fetched score ; do
        # Expire fetched pages in 30 days
        expire_date=$[${date_fetched} + 2592000]
        curr_date=`date +%s`

        if [ "${expire_date}" -gt "${curr_date}" ] ; then
                echo "${hostname} ${filename} ${date_fetched} ${score}" >> "${dir}/fetched-2"
        else
                echo "- Purging ${dir}/${filename}..."
                rm -f "${dir}/${filename}" "${dir}/${filename}.log"
        fi
done < "${dir}/fetched"
if [ -f "${dir}/fetched-2" ] ; then
        mv -f "${dir}/fetched-2" "${dir}/fetched"
fi

num_urls=`wc -l "${url_file}" | cut -d" " -f1`
i=0

# Fetch and process requested pages
while read hostname ; do
        i=$[${i} + 1]

        # Check to see if host needs to be fetched
        if grep -qw ${hostname} "${dir}/fetched" ; then
                echo "[${i}/${num_urls}] Skipping ${hostname}"
                continue
        fi
        
        # Normalize hostname
        hostname=`echo ${hostname} | tr ' ' '_'`
        hostfile=`echo ${hostname} | sed -e 's/[^-.A-Za-z0-9]/_/g'`

        echo "[${i}/${num_urls}] Fetching ${hostname}..."
        (
           "${curl_bin}" --connect-timeout 5 --max-time 10 --location ${hostname} | \
           "${lynx_bin}" -dump -stdin -force_html -nolist -noprint > "${dir}/.temp"
        ) 1>&2 2> /dev/null

        # Normalize the text into lowercase newline
        # delimited words:
        #   * Convert all but a small subset of characters to spaces
        #   * Remove leading and trailing spaces
        #   * Compress repeated spaces
        #   * Put each term on a separate line
        #   * Delete empty lines
        #   * Lowercase all characters
        cat "${dir}/.temp" | \
           sed -e 's/[^-A-Za-z0-9 ]/ /g' | \
           sed -e 's/^[ ]*//g;s/[ ]*$//g' | \
           sed -e 's/[ ][ ]*/ /g' | \
           tr ' ' '\n' | \
           sed -e '/^$/d' | \
           tr 'A-Z' 'a-z' > "${dir}/${hostfile}.txt"

        # Abort if we didn't end up with any data
        if [ -z "${dir}/${hostfile}.txt" ] ; then
                rm -f "${dir}/${hostfile}.txt"
                continue
        fi

        # Call vsm and test return code
        ${vsm_bin} -t "${term_file}" "${dir}/${hostfile}.txt" 1> "${dir}/.temp" 2> "${dir}/${hostfile}.txt.log"
        if [ ${?} -ne 0 ] ; then
                echo "Error: Non-zero exit code returned from vsm"
                rm -f "${dir}/${hostfile}.txt"
                continue
        fi
 
        score=`cat "${dir}/.temp" | cut -d" "  -f2`
        date_fetched=`date +%s`
        echo "${hostname} ${hostfile}.txt ${date_fetched} ${score}" >> "${dir}/fetched"

        rm -f "${dir}/.temp"
done < ${url_file}
